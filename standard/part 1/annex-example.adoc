[appendix]
[[annex-example]]
== Example (Informative)

=== TrainingDataset encoding examples

==== WHU-RS19 dataset

https://captain-whu.github.io/BED4RS/[The WHU-RS19 dataset] is widely used in scene classification of remote sensing images. This dataset is collected from Google Earth and has 19 classes including airport, beach, bridge, commercial, desert, farmland, football field, forest, industrial, meadow, mountain, park, parking, pond, port, railway station, residential, river, and viaduct. Each class contains around 50 images, with the image size 600×600 and a resolution of 0.5m.

An example of JSON encoding of the WHU-RS19 dataset following the TrainingDML-AI UML model can be found in https://gitlab.ogc.org/ogc/TrainingDML-AI/-/blob/master/use-cases/examples/WHU-RS19.json.

==== DOTA-v1.5 dataset

https://captain-whu.github.io/DOTA/[The DOTA-v1.5 dataset] is a large-scale dataset for object detection in aerial images. The sources for content in the dataset include Google Earth, Gaofen-2, and Jilin-1 imagery provided by China Resources Satellite Data Center. The 16 classes in DOTA-v1.5 are plane, ship, storage tank, baseball diamond, tennis court, basketball court, ground track field, harbor, bridge, large vehicle, small vehicle, helicopter, roundabout, soccer ball field, swimming pool, and container crane. Compared with other aerial image object detection datasets, the dataset has the largest number of classes. The images in the dataset have various image sizes (from 800×800 to 2000×2000) and resolutions (Google Earth/0.1m-1m, Gaofen-2/1m, Jilin-1/0.72m).

An example of JSON encoding of the DOTA-v1.5 dataset following the TrainingDML-AI UML model can be found in https://gitlab.ogc.org/ogc/TrainingDML-AI/-/blob/master/use-cases/examples/DOTA-v1.5.json.

==== KITTI 2D object detection dataset

http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=2d[The KITTI 2D object detection dataset] is a novel open-access dataset and benchmark for road area and ego-lane detection. KITTI 2D  consists of 7481 annotated training images of high variability from the KITTI autonomous driving platform by 2 PointGrey Flea2 color cameras, capturing a broad spectrum of urban street views and road scenes. The eight (8) classes in the KITTI 2D object detection dataset are car, van, truck, pedestrian, person_sitting, cyclist, tram, and misc. Compared with other street view object detection datasets, this dataset compresses diverse scenarios and captures real-world traffic situations, ranging from freeways over rural areas to inner-city scenes with many static and dynamic objects.

An example of JSON encoding of the KITTI 2D object detection dataset following the TrainingDML-AI UML model can be found in https://gitlab.ogc.org/ogc/TrainingDML-AI/-/blob/master/use-cases/examples/KITTI.json.

==== GID dataset

https://x-ytong.github.io/project/GID.html[The GID dataset] is one of start-of-art land cover classification datasets. This dataset has a large spatial coverage covering many provinces in China with a relatively high spatial resolution (2m). GID has two sets. One is the GID-5C. It has 150 images (image size 7200×6800) that are classified into 5 land cover classes. The other set is GID-15C. The images from GID-5C are sliced into 30,000 patches in GID-15C, which have three types of patch sizes (56×56, 112×112, 224×224) and are classified into 15 land cover classes.

An example of JSON encoding of the GID-5C dataset following the TrainingDML-AI UML model can be found in https://gitlab.ogc.org/ogc/TrainingDML-AI/-/blob/master/use-cases/examples/GID-5C.json.

==== Toronto3D dataset

https://github.com/WeikaiTan/Toronto-3D[The Toronto3D dataset] is a large urban outdoor point cloud dataset for segmentation collected by the Mobile Laser Scanning System. The dataset covers about 1 km of scene streets in Toronto, including four areas named L001, L002, L003, and L004, with a total of 78.3 million points. Each point in this dataset has 10 attributes representing the 3D position, RGB color, intensity, GPS time, scan angle rank, and category, respectively. This dataset has eight categories, including road, road mark, natural, building, utility line, pole, car, and fence.

An example of JSON encoding of the Toronto3D dataset following the TrainingDML-AI UML model can be found in https://gitlab.ogc.org/ogc/TrainingDML-AI/-/blob/master/use-cases/examples/Toronto_3D.json.

==== WHU-Building dataset

http://gpcv.whu.edu.cn/data/building_dataset.html[The WHU-Building dataset] is a change detection dataset collected from the Land Information New Zealand Data Service. The dataset is composed of images (with the resolution 0.2m) in 2012 and 2016, covering 20.5 km2. It includes 12,796 and 16,077 buildings respectively in 2012 and 2016.

An example of JSON encoding of the WHU-Building dataset following the TrainingDML-AI UML model can be found in https://gitlab.ogc.org/ogc/TrainingDML-AI/-/blob/master/use-cases/examples/WHU-building.json.

==== California change detection dataset

https://arxiv.org/abs/1909.05948[The California Change Detection Dataset] is composed of two images and a label image. The first image is a Landsat 8 acquisition covering Sacramento County, Yuba County and Sutter County, California, on 5 January 2017. It has nine channels covering the spectrum from deep blue to short-wave infrared, plus two long-wave infrared channels. The second image was acquired on 18 February 2017 by Sentinel-1A over the same area after the occurrence of a flood. The image is recorded in polarizations VV and VH and augmented with the ratio between the two intensities as a third channel. All these channels are log-transformed.

An example of JSON encoding of the California change detection dataset following the TrainingDML-AI UML model can be found in https://gitlab.ogc.org/ogc/TrainingDML-AI/-/blob/master/use-cases/examples/UiT_HCD_California_2017.json.

==== WHU MVS dataset

http://gpcv.whu.edu.cn/data/WHU_MVS_Stereo_dataset.html[The WHU MVS dataset] is a synthetic aerial dataset created for large-scale and high-resolution Earth surface reconstruction. The basic training sample of the dataset is a multi-view unit consisting of five aerial images, and their corresponding depth maps are taken as ground truth. There are a total of 5680 pairs of five-view aerial images in the dataset. All the images are simulated from a 3D surface model, which is produced by Smart3D software using Unmanned Aerial Vehicle (UAV) images and refined by manual editing.

An example of JSON encoding of the WHU MVS dataset following the TrainingDML-AI UML model can be found in https://gitlab.ogc.org/ogc/TrainingDML-AI/-/blob/master/use-cases/examples/WHU_MVS.json.

=== DataQuality encoding example

==== WHU-RS19 data quality

An encoded data quality example of the WHU-RS19 datasets following the TrainingDML-AI UML model can be found in https://gitlab.ogc.org/ogc/TrainingDML-AI/-/blob/master/use-cases/examples/WHU-RS19-quality.json.

=== TDChangeset encoding examples

==== DOTA-v1.5 changeset

DOTA-v1.5 uses the same images as DOTA-v1.0, but the extremely small instances (less than 10 pixels) are also annotated. Moreover, a new category “container crane” is added. It contains 403,318 instances in total. The number of images and dataset splits are the same as DOTA-v1.0. This version was released for the DOAI Challenge 2019 on Object Detection in Aerial Images in conjunction with IEEE CVPR 2019.

An encoded changeset example between the DOTA-v1.0 and DOTA-v1.5 datasets following the TrainingDML-AI UML model can be found in https://gitlab.ogc.org/ogc/TrainingDML-AI/-/blob/master/use-cases/examples/DOTA-v1.5-changeset.json.